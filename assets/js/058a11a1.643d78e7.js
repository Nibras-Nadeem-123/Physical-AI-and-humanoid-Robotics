"use strict";(globalThis.webpackChunkphysical_ai_and_humanoid_robotic_textbook=globalThis.webpackChunkphysical_ai_and_humanoid_robotic_textbook||[]).push([[7735],{5680:(e,t,n)=>{n.d(t,{xA:()=>u,yg:()=>m});var a=n(6540);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter(function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable})),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach(function(t){r(e,t,n[t])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach(function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))})}return e}function l(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var s=a.createContext({}),c=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},u=function(e){var t=c(e.components);return a.createElement(s.Provider,{value:t},e.children)},d="mdxType",g={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},p=a.forwardRef(function(e,t){var n=e.components,r=e.mdxType,o=e.originalType,s=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),d=c(n),p=r,m=d["".concat(s,".").concat(p)]||d[p]||g[p]||o;return n?a.createElement(m,i(i({ref:t},u),{},{components:n})):a.createElement(m,i({ref:t},u))});function m(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=n.length,i=new Array(o);i[0]=p;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[d]="string"==typeof e?e:r,i[1]=l;for(var c=2;c<o;c++)i[c]=n[c];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}p.displayName="MDXCreateElement"},5736:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>i,default:()=>g,frontMatter:()=>o,metadata:()=>l,toc:()=>c});var a=n(8168),r=(n(6540),n(5680));const o={id:"07-vla",title:"Vision-Language-Action Models (VLAs)",sidebar_label:"VLA Models"},i="Chapter 7: Vision-Language-Action Models (VLAs)",l={unversionedId:"chapters/07-vla",id:"chapters/07-vla",title:"Vision-Language-Action Models (VLAs)",description:'Vision-Language-Action (VLA) models represent the cutting edge of AI for robotics, enabling robots to understand and execute high-level, natural language commands. This chapter delves into the architecture and application of these powerful "robot brain" models.',source:"@site/docs/chapters/07-vla.mdx",sourceDirName:"chapters",slug:"/chapters/07-vla",permalink:"/Physical-AI-and-humanoid-Robotics/docs/chapters/07-vla",draft:!1,editUrl:"https://github.com/ai-driven-development/physical-ai-and-humanoid-robotic-textbook/tree/main/docs/chapters/07-vla.mdx",tags:[],version:"current",sidebarPosition:7,frontMatter:{id:"07-vla",title:"Vision-Language-Action Models (VLAs)",sidebar_label:"VLA Models"},sidebar:"chapters",previous:{title:"NVIDIA Isaac Sim",permalink:"/Physical-AI-and-humanoid-Robotics/docs/chapters/06-nvidia-isaac"},next:{title:"Humanoid Design & Control",permalink:"/Physical-AI-and-humanoid-Robotics/docs/chapters/08-humanoids"}},s={},c=[{value:"7.1 The Convergence of Vision, Language, and Action",id:"71-the-convergence-of-vision-language-and-action",level:2},{value:"7.2 Architecture of a VLA",id:"72-architecture-of-a-vla",level:2},{value:"7.3 Training VLAs: From Imitation to Reinforcement",id:"73-training-vlas-from-imitation-to-reinforcement",level:2},{value:"7.4 RT-1, RT-2, and the Transformer Revolution in Robotics",id:"74-rt-1-rt-2-and-the-transformer-revolution-in-robotics",level:2},{value:"7.5 Practical Implementation: Running a Pre-trained VLA",id:"75-practical-implementation-running-a-pre-trained-vla",level:2}],u={toc:c},d="wrapper";function g({components:e,...t}){return(0,r.yg)(d,(0,a.A)({},u,t,{components:e,mdxType:"MDXLayout"}),(0,r.yg)("h1",{id:"chapter-7-vision-language-action-models-vlas"},"Chapter 7: Vision-Language-Action Models (VLAs)"),(0,r.yg)("p",null,'Vision-Language-Action (VLA) models represent the cutting edge of AI for robotics, enabling robots to understand and execute high-level, natural language commands. This chapter delves into the architecture and application of these powerful "robot brain" models.'),(0,r.yg)("h2",{id:"71-the-convergence-of-vision-language-and-action"},"7.1 The Convergence of Vision, Language, and Action"),(0,r.yg)("p",null,"We explore the motivation behind VLAs. By grounding large language models (LLMs) in visual data and robotic actions, we can create systems that have a much deeper, more functional understanding of the world."),(0,r.yg)("h2",{id:"72-architecture-of-a-vla"},"7.2 Architecture of a VLA"),(0,r.yg)("p",null,"This section breaks down the typical components of a VLA:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},"Vision Encoder:")," A model (like ViT) that processes camera input and extracts visual features."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},"Language Encoder:")," A model that processes the text-based command."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},"Fusion Layer:")," A mechanism (e.g., cross-attention) that combines the vision and language features."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},"Action Decoder:")," A policy network that outputs the low-level motor commands (e.g., joint torques or end-effector poses) based on the fused representation.")),(0,r.yg)("h2",{id:"73-training-vlas-from-imitation-to-reinforcement"},"7.3 Training VLAs: From Imitation to Reinforcement"),(0,r.yg)("p",null,"Training these large models is a significant challenge. We will discuss the primary methods:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},"Behavioral Cloning (Imitation Learning):")," Training the model on large datasets of human teleoperated demonstrations."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},"Reinforcement Learning from Human Feedback (RLHF):")," Fine-tuning the model based on human preferences for different robot behaviors."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},"Sim-to-Real Transfer:")," Pre-training the model in simulation and then fine-tuning it on a smaller amount of real-world data.")),(0,r.yg)("h2",{id:"74-rt-1-rt-2-and-the-transformer-revolution-in-robotics"},"7.4 RT-1, RT-2, and the Transformer Revolution in Robotics"),(0,r.yg)("p",null,"We will study influential VLA architectures from Google DeepMind, such as RT-1 and RT-2. We'll examine how the transformer architecture, originally developed for natural language processing, has been adapted to process sequences of sensor data and output sequences of actions."),(0,r.yg)("h2",{id:"75-practical-implementation-running-a-pre-trained-vla"},"7.5 Practical Implementation: Running a Pre-trained VLA"),(0,r.yg)("p",null,'This section will guide you through the process of downloading a pre-trained VLA and integrating it with your ROS 2-based robot control system. You\'ll be able to give your simulated robot natural language commands like "pick up the red block" and see it execute the task.'))}g.isMDXComponent=!0}}]);